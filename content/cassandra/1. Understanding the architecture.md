# gossip

A peer-to-peer communication protocol to discover and share **location and state information** about the other nodes in a Cassandra cluster. 
Gossip information is also persisted locally by each node to use immediately when a node restarts.


Gossip is a peer-to-peer communication protocol in which nodes periodically exchange state information about themselves and about other nodes they 
know about. The gossip process runs **every second** and exchanges state messages with **up to three** other nodes in the cluster. The nodes exchange
information about **themselves and about the other nodes** that they have gossiped about, so all nodes quickly learn about all other nodes in the cluster. 
A gossip message has a version associated with it, so that during a gossip exchange, older information is overwritten with the most current state for a 
particular node.

To prevent problems in gossip communications, use the same list of seed nodes for all nodes in a cluster. This is most critical the first time a node starts up. By default, a node remembers other nodes it has gossiped with between subsequent restarts. The seed node designation has no purpose other than bootstrapping the gossip process for new nodes joining the cluster. Seed nodes are not a single point of failure, nor do they have any other special purpose in cluster operations beyond the bootstrapping of nodes.

Gossip信息会被node保存到本地，在重启时使用；Gossip每秒运行一次，和最多其它三个节点交换本机和其它Nodes的信息，所以所有的Node在很短时间内就可以知道整个cluster的Node信息；

在使用多数据中心集群时，Seed list中必须包含各数据中心至少一台机器，多于一台可以用来容错，但是没必要包含所有的node，这会降低性能，seed list应该尽可能小；


# Failure detection and recovery 

Failure detection is a method for locally determining from **gossip state and history** if another node in the system is **down or has come back up**. 
Cassandra uses this information to avoid routing client requests to unreachable nodes whenever possible. (Cassandra can also avoid routing requests to nodes
that are alive, but performing poorly, through the **dynamic snitch**.)

Cassandra使用Gossip状态或历史数据来判断Node是否Down或恢复，避免将请求发送到失败的Node，同时根据dynamic snitch避免将数据发送到poor性能的Node(如果数据有多个副本，通过dynaic snitch
可以像最好的node请求数据，也可以先将数据发送到较好的node，后续通过repair功能将数据复制到其它副本Node上)；

The gossip process tracks state from other nodes both directly (nodes gossiping directly to it) and indirectly (nodes communicated about secondhand, 
third-hand, and so on). Rather than have a fixed threshold for marking failing nodes, Cassandra uses an accrual detection mechanism to calculate a per-node 
threshold that takes into account network performance, workload, and historical conditions. During gossip exchanges, every node maintains a sliding window of 
**inter-arrival times** of gossip messages from other nodes in the cluster. Configuring the **phi_convict_threshold property** adjusts the sensitivity of the 
failure detector. Lower values increase the likelihood that an unresponsive node will be marked as down, while higher values decrease the likelihood that 
transient failures causing node failure. Use the default value for most situations, but increase it to 10 or 12 for Amazon EC2 (due to frequently encountered
network congestion). In unstable network environments (such as EC2 at times), raising the value to 10 or 12 helps prevent false failures. Values higher than 
12 and lower than 5 are not recommended.

Node failures can result from various causes such as hardware failures and network outages. Node outages are often transient but can last for extended periods.
Because a node outage rarely signifies a permanent departure from the cluster it **does not** automatically result in permanent removal of the node from the 
ring.Other nodes will **periodically try to re-establish** contact with failed nodes to see if they are back up. To permanently change a node's membership in a
cluster,administrators **must explicitly add or remove** nodes from a Cassandra cluster using the nodetool utility.

When a node comes back online after an outage, it may have missed writes for the replica data it maintains. Repair mechanisms exist to **recover missed data**, 
such as **hinted handoffs and manual repaeir** with nodetool repair. The length of the outage will determine which repair mechanism is used to make the data 
consistent.

当一个node离线时，cassandra并不自动从ring中删除它们，而是周期的尝试建立连接，如果节点恢复后，通过恢复机制(如hinted handoffs和手动修复)来弥补丢失的数据；
管理员需要手动的添加或移除Node；

  
# Data distribution and replication 

In Cassandra, data distribution and replication go together. 
Data is organized by table and identified by a primary key, which determines which node the data is stored on. Replicas are copies of rows. 
When data is first written, it is also referred to as a replica.

Cassandra中数据分配和复制是一起进行的。数据按照table组织，被PK所标识，Partion key决定了数据应该被分配到哪个Node上，复制机制将数据Row复制到其它多个Nodes上；

Factors influencing replication include:
+ Virtual nodes: assigns data ownership to physical machines.
+ Partitioner: partitions the data across the cluster.
+ Replication strategy: determines the replicas for each row of data.
+ Snitch: defines the topology information that the replication strategy uses to place replicas.

## Consistent hashing

Consistent hashing allows distribution of data across a cluster to **minimize reorganization** when nodes are added or removed. Consistent hashing partitions
data based on the **partition key**. (For an explanation of partition keys and primary keys, see the Data modeling example in CQL for Cassandra 2.2 and later.)

1. Cassandra assigns a hash value to each partition key；
2. Each node in the cluster is responsible for a range of data based on the hash value；
3. Cassandra places the data on each node according to the value of the partition key and the range that the node is responsible for. 

简单分区策略放置数据的方法：
1. Cassandra 给每个data row的partition key计算一个hash值；
2. 集群中的每个Node负责一个range的hash值范围(在cassnadra.yaml配置文件中的initial_token参数指定)；
3. cassandra根据data的hash值和各Node的 hash range，将数据放置到对应的Node上；
4. 复制策略是，顺时针将数据放置到后续多个Node上(取决于复制因子)；

上面没有使用vnode机制，每个Node只包含一个Hash range，range值需要在配置文件中指定；

# Partition key 和 Primary Key

In the table definition, a **clustering column** is a column that is part of the **compound primary key definition**, but not the **first column, which is position reserved for the partition key**. 
Columns are clustered in multiple rows within a single partition. The clustering order is determined by the position of columns in the compound primary key definition.

一个更复杂一点的例子:

CREATE TABLE example (
    partitionKey1 text,
    partitionKey2 text,
    clusterKey1 text,
    clusterKey2 text,
    normalField1 text,
    normalField2 text,
    PRIMARY KEY (
        (partitionKey1, partitionKey2),
        clusterKey1, clusterKey2
        )
    );

这里我们用字段在内部存储时的类型来命名字段。而且我们已经包含了所有情形。这里的主键不仅仅是**复合主键**，还是复合**partition key** （PRIMARY KEY的第一个字段，用括号括起的部分)和**复合cluster key**。
存储方式：
RowKey: partitionVal1:partitionVal2   // partition key的组合；
=> (column=clusterVal1:clusterVal2:, value=, timestamp=1374630892473000) 
=> (column=clusterVal1:clusterVal2:normalfield1, value=6e6f726d616c56616c31, timestamp=1374630892473000) // cluster keys的values和其它filedname组合成一个colum；
=> (column=clusterVal1:clusterVal2:normalfield2, value=6e6f726d616c56616c32, timestamp=1374630892473000)

**对于复合主键，第一个字段为partition key，后续字段为cluster key；**

# Vnodes

Every **partition key** in Cassandra is converted to **a numerical token** value using the MurMur3 hash function. The token range is between -2^63 to +2^63.
num_token defines how many token ranges are assigned to a node. Each node calculates 256 (num_tokens) **random values** in the token range and 
**informs other nodes** what they are, thus when a node needs to coordinate a request for a specific token it knows which nodes are responsible for it, 
according to the Replication Factor and DC/rack placement. 

A better description for this feature would be "automatic token range assignment for better streaming capabilities",
calling it "virtual" is a bit confusing. In your case you have 6 nodes, each set with 256 token ranges so you have 6*256 token ranges and each psychical node
contains 256 token ranges.

For example consider 2 nodes with num_tokens set to 4 and token range 0 to 100. Node 1 calculates tokens 17, 35, 77, 92 Node 2 calculates tokens 4, 25, 68, 85
The ring shows the distribution of token ranges in this case Node 2 is responsible for token ranges 4-17, 25-35, 68-77, 85-92 and node 1 for the rest.

Partition ranges are determined by granting each node the range from their available tokens up until the next specified token.
Data is **exchanged through gossip** detailing which nodes have which tokens. This meta-data allows every node to know which nodes are responsible for which ranges. 
Keyspace/Replication settings also change where data is actually saved.

EXAMPLE: 
    1)A gets 256 ranges, B gets 256 Ranges. But to make this simple lets give them each 2 tokens and pretend the token range is 0 to 30
    Given tokens: A 10,15 and B 3,11 Nodes are responsible for the following ranges
        (3-9:B)(10:A)(11-14:B)(15-30,0-2:A)
    If C Joins also with 2 tokens 20,5 Nodes will now be responsible for the following ranges
        (3-4:B)(5-9:C)(10:A)(11-14:B)(15-19:A)(20-30,0-2:C)

Vnodes are powerful because now when C joins the cluster it **gets its data from multiple nodes** (5-9 from B and 20-30,0-2 from A) sharing the load
between those machines. In this toy example you can see that having only 2 tokens allows for some nodes to host the majority of the data while others
get almost none. As the number of Vnodes increases the balance between the nodes increases as the ranges become randomly subdivided more and more. 
At 256 nodes you are extremely likely to have distributed an even amount of data to each node in the cluster.

每个物理Node在配置文件中指定 num_tokens 参数，启动后在整个Hash Range中随机分配对应数目的range，使用gossip协议和其它Node通信，从而了解分配的range在其它Node是否有数据，
如果有，则从对应的Node获取range对应的数据，同时对端Node自动调整自己的range（去掉其它Node分配的范围）；

使用vnode的优势：
1. Tokens are automatically calculated and assigned to each node.
2. Rebalancing a cluster is automatically accomplished when adding or removing nodes. When a node joins the cluster, it assumes responsibility for an even portion of data from the other nodes in the cluster. If a node fails, the load is spread evenly across other nodes in the cluster.
3. Rebuilding a dead node is faster because it involves every other node in the cluster.
4. The proportion number of vnodes assigned to each machine in a cluster can be assigned, so smaller and larger computers can be used in building a cluster.

# Multil DataCenter

When using the NetworkTopologyStrategy, you specify your **replication factor** on a per-data-center basis using 
strategy_options:{data-center-name}={rep-factor-value} rather than the global strategy_options:replication_factor={rep-factor-value}.

Here's a concrete example adapted from http://www.datastax.com/docs/1.0/references/cql/CREATE_KEYSPACE

CREATE KEYSPACE Excalibur WITH strategy_class = 'NetworkTopologyStrategy' AND strategy_options:DC1 = 2 AND strategy_options:DC2 = 2;

In that example, any given column would be stored on 4 nodes total, with 2 in each data center.

如果是 mutil datacener，则在创建 KEYSPACE 时需要使用 NetworkTopologyStrategy 策略，同时分别指定各数据中心的复制数目；总的复制数目是指定的各数据中心复制数据的总和；
数据中心内的各Node组成一个replication group；
 
# Data Replication

In a distributed system like Cassandra, data replication enables **high availability and durability**. Cassandra replicates rows in a column family on to 
multiple endpoints based on the replication strategy associated to its keyspace. The endpoints which store a row are called **replicas or natural endpoints**
for that row.

Partitioner和各Node分配的vnodes决定了数据应该写到哪个Node(partitioner根据partition key计算出hash值，判断该hash值所需的vnode(根据Gossip协议获取各Node包含的vnode信息)，进而
找到所对应的Node)；Replication Strategy 决定了数据应该被复制到哪些节点；
 
Number of replicas and their location are determined by **replication factor and replication strategy**. Replication strategy controls how the replicas are 
chosen and replication factor determines the number of replicas for a key. Replication strategy is defined when creating a keyspace and replication factor 
is configured differently based on the chosen replication strategy.

Two kinds of replication strategies available in Cassandra. First one is **SimpeStrategy** which is rack unaware and data center unaware policy. It is commonly 
used when nodes are in a single data center. 

The second strategy is **NetworkTopologyStategy** which is both rack aware and data center aware. Even if you have single data center but nodes are in different 
racks it is better to use NetworkTopologyStategy which is rack aware and enables fault tolerance by choosing replicas from different racks. Also if you are 
planning to expand the cluster in the future to have more than one data center then choosing NetworkTopologyStategy from the beginning avoids data migration.

Data partitioner determines **coordinator node** for each key. The coordinator node is **the fist** replica for a key which is also called **primary replica**. 
If replication factor is N, a **key's coordinator node** replicates it to other N-1 replicas.


In SimpleStrategy, successor nodes or the nodes on the ring immediate following in clockwise direction to the coordinator node are selected as replicas. 

In NetworkTopologyStategy, nodes from distinct available racks in each data center are chosen as replicas. User need to specify per data center replication 
factor in multiple data center environment. In each data center, successor nodes to the coordinator node which are from distinct racks are chosen. 

Cassandra nodetool getendpoints command can be used to retrieve the replicas of a key.

coordinator node是被Data partitioner选择的保存数据第一个replica(primary replice)的Node，它同时负责将数据Copy到其它的Nodes；
SimpleStrategy 选中coordinator后面顺时钟方向的Nodes为Replicas；
NetworkTopologyStategy 需要为各数据中心指定复制因子，选中在同一个数据中内部coordinator后面顺时钟方向的不同Rack的Nodes为Replicas；

**Asymmetrical replication groupings** are also possible. For example, you can have three replicas in one datacenter to serve real-time application requests and 
use a single replica elsewhere for running analytics.

# Replication factor

The total number of replicas across the cluster. A replication factor of 1 means that there is only one copy of each row on one node. 
A replication factor of 2 means two copies of each row, where each copy is on a **different** node. All replicas are **equally** important; there is no primary or 
master replica. You define the replication factor **for each datacenter**. Generally you should set the replication strategy greater than one, but no more than 
the number of nodes in the cluster.

复制因子指的是cluster范围内的的副本数量；
当复制因子为2时，将在一个node上放置数据；
数据的所有的副本都是同等重要，没有主次之分；
可以为每个数据中心定义复制因子，keyspace的总副本数等于各数据中心的副本数之和；为keyspace指定跨数据中的复制因子，可以实现跨数据中心的高可用；


# Replica placement strategy

Cassandra stores copies (replicas) of data on multiple nodes to ensure reliability and fault tolerance. 
A replication strategy determines which nodes to place replicas on. The first replica of data is simply the first copy; it is not unique in any sense. 
The **NetworkTopologyStrategy** is highly recommended for most deployments because it is much easier to expand to multiple datacenters when required by future 
expansion.

When creating a keyspace, you must define the replica placement strategy and the number of replicas you want.

复制因子决定了副本数目，复制放置策略决定了各副本在哪些Node上放置；建议使用NetworkTopologyStrategy放置策略，它在放置副本的时候考虑 数据中心、机柜 属性(snitch提供各Node的这些属性信息)，
尽量将副本放到下一个不同机柜上；

# Partitioner

A partitioner determines which node will receive **the first replica** of a piece of data, and how to distribute other replicas across other nodes in the cluster. 
Each row of data is uniquely identified by a **primary key**, which may be the same as its **partition key**, but which may also include other **clustering columns**. 

A partitioner is a hash function that derives a **token** from the primary key of a row. The partitioner uses the token value to determine which nodes in the 
cluster receive the replicas of that row. The Murmur3Partitioner is the default partitioning strategy for new Cassandra clusters and the right choice for 
new clusters in almost all cases.

You must set the partitioner and assign the node a **num_tokens** value for each node. The number of tokens you assign depends on the hardware capabilities 
of the system. If not using virtual nodes (vnodes), use the **initial_token** setting instead.

Partitioner决定哪个node收到data的第一份copy，同时复制到其它Nodes；使用vnode时(默认)，需要在配置文件中指定 num_tokens参数，否则使用initial_token参数；

Both the **Murmur3Partitioner** and **RandomPartitioner** use tokens to help assign **equal portions of data** to each node and evenly distribute data from 
all the tables throughout the ring or other grouping, such as a keyspace. This is true even if the tables use different partition keys, such as usernames or 
timestamps. Moreover, the read and write requests to the cluster are also evenly distributed and load balancing is simplified because each part of the hash 
range receives an equal number of rows on average. For more detailed information, see Consistent hashing.

The main difference between the two partitioners is how each generates the token hash values. The RandomPartitioner uses a cryptographic hash that 
**takes longer** to generate than the Murmur3Partitioner. Cassandra doesn't really need a cryptographic hash, so using the Murmur3Partitioner results in a 
3-5 times improvement in performance.

Cassandra offers the following partitioners that can be set in the cassandra.yaml file.

1. Murmur3Partitioner (default): uniformly distributes data across the cluster based on MurmurHash hash values.
2. RandomPartitioner: uniformly distributes data across the cluster based on MD5 hash values.
3. ByteOrderedPartitioner: keeps an ordered distribution of data lexically by key bytes

The Murmur3Partitioner is the default partitioning strategy for Cassandra 1.2 and later new clusters and the right choice for new clusters in almost all 
cases. However, the partitioners are not compatible and data partitioned with one partitioner cannot be easily converted to the other partitioner.

Note: If using virtual nodes (vnodes), you do not need to calculate the tokens. If not using vnodes, you must calculate the tokens to assign to the 
initial_token parameter in the cassandra.yaml file. See Generating tokens and use the method for the type of partitioner you are using.

# Snitch(告密)

A snitch defines groups of machines into datacenters and racks (the topology) that the replication strategy uses to place replicas.
Cassandra **does its best** not to have more than one replica on the same rack (which is not necessarily a physical location).

1. Dynamic snitching
Monitors the performance of **reads** from the various replicas and chooses the best replica based on this history.
2. SimpleSnitch
The SimpleSnitch is used only for single-datacenter deployments.
3. RackInferringSnitch
Determines the location of nodes by rack and datacenter corresponding to the **IP** addresses.
4. PropertyFileSnitch
Determines the location of nodes by rack and datacenter. 
在配置文件cassandra-topology.properties中定义各Node的IDC和Rack信息；
5. GossipingPropertyFileSnitch
Automatically updates all nodes using gossip when adding new nodes and is recommended for production.
在配置文件 cassandra-rackdc.properties中定义本Node的IDC和Rack信息；

可以从 PropertyFileSnitch 迁移到 GossipingPropertyFileSnitch；GossipingPropertyFileSnitch总是加载cassandra-topology.properties，所以在迁移后，需要删除该文件；

You must configure a snitch when you create a cluster. All snitches use **a dynamic snitch layer**, which monitors performance and chooses the best replica 
**for reading**. It is enabled by default and recommended for use in most deployments. Configure dynamic snitch thresholds for each node in the cassandra.yaml 
configuration file.

The **default SimpleSnitch** does not recognize datacenter or rack information. Use it for single-datacenter deployments or single-zone in public clouds. 
The **GossipingPropertyFileSnitch** is recommended for production. It defines a node's datacenter and rack and uses gossip for propagating this information 
to other nodes.

snitch 用于将各Node按照数据中心、机柜进行分组，复制放置策略(NetworkTopologyStrategy)使用这个分组信息决定如何放置副本；
dynamic snitch监控各node的性能，在读取操作(非write操作)时，选择最合适的的副本；
建议使用GossipingPropertyFileSnitch，它将在Node配置文件中定义的数据中心、机柜信息传递到其它node；

If you change snitches, you may need to perform additional steps because the **snitch affects where replicas are placed**. See Switching snitches.

1. Dynamic snitching 

By default, **all snitches also use a dynamic snitch layer** that monitors read latency and, when possible, routes requests away from poorly-performing nodes.
The dynamic snitch is enabled by default and is recommended for use in most deployments. 
For information on how this works, see Dynamic snitching in Cassandra: past, present, and future. 
Configure dynamic snitch thresholds for each node in the cassandra.yaml configuration file.

http://www.datastax.com/dev/blog/dynamic-snitching-in-cassandra-past-present-and-future

# The cassandra.yaml configuration file

cassandra的配置文件，定义了cluster、caching相关的参数，也定义了commitlog、sstable 文件的位置；

按key，value储存API返回结果
更新API完成状态，通过依赖关系找到下一个API并调度执行
自动根据传入参数和依赖关系找到下一个API所需要的参数