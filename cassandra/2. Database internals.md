1. Storage engine
A description about Cassandra's storage structure and engine.

2. How Cassandra reads and writes data
Understanding how Cassandra stores data.

3. Data consistency
Topics about how up-to-date and synchronized a row of data is on all replicas.

# Storage engine

Cassandra uses a storage structure similar to a **Log-Structured Merge Tree**, unlike a typical relational database that uses a B-Tree. 
Cassandra avoids reading before writing. Read-before-write, especially in a large distributed system, can result in large latencies in read performance and 
other problems. For example, two clients read at the same time; one overwrites the row to make update A, and the other overwrites the row to make update B, 
removing update A. This race condition will result in ambiguous query results - which update is correct?

To **avoid using read-before-write for most writes** in Cassandra, the storage engine **groups inserts and updates in memory**, and at intervals, 
sequentially **writes the data to disk in append mode**. Once written to disk, the data is immutable and is never overwritten. Reading data involves combining 
this immutable sequentially-written data to discover the correct query results. You can use **Lightweight transactions (LWT)** to check the state of the data 
before writing. However, this feature is recommended only for limited use.

A log-structured engine that avoids overwrites and uses sequential I/O to update data is essential for writing to solid-state disks (SSD) and hard disks (HDD).
On HDD, writing randomly involves a higher number of seek operations than sequential writing. The seek penalty incurred can be substantial. 
Because Cassandra sequentially writes immutable files, thereby avoiding write amplification and disk failure, the database accommodates inexpensive, 
consumer SSDs extremely well. For many other databases, **write amplification is a problem on SSDs**.

Cassandra 使用LSMT存储架构，在write时避免 read-before-write，方法是：数据首先在内存中插入或更新，周期用append模式的串行IO写入到磁盘，一旦写入磁盘将不能再更新；
读数据时需要综合磁盘和内存中的数据来获得最终结果；

# How Cassandra reads and writes data

To manage and access data in Cassandra, it is important to understand how Cassandra stores data. 
The **hinted handoff** feature plus Cassandra conformance and non-conformance to the ACID (atomic, consistent, isolated, durable) database properties are key 
concepts to understand reads and writes. In Cassandra, consistency refers to how up-to-date and synchronized a row of data is on all of its replicas.

Client utilities and application programming interfaces (APIs) for developing applications for data storage and retrieval are available.

## How is data written?

Cassandra processes data at several stages on the write path, starting with the immediate logging of a write and ending in with a write of data to disk:
1. Logging data in the commit log
2. Writing data to the memtable
3. Flushing data from the memtable
4. Storing data on disk in SSTables


1. Logging writes and memtable storage 

When a write occurs, Cassandra stores the data in a memory structure called **memtable**, and to provide configurable durability, it also **appends** writes 
to the **commit log** on disk. The commit log receives every write made to a Cassandra node, and these durable writes survive permanently even if power fails 
on a node. The memtable is a **write-back cache** of data partitions that Cassandra looks up by key. The memtable stores writes in sorted order until reaching
a configurable limit, and then is flushed.

2. Flushing data from the memtable 

To flush the data, Cassandra writes the data to disk, in the **memtable-sorted order**.. A **partition index** is also created on the disk that maps the tokens 
to a location on disk. When the memtable content exceeds the configurable threshold or the commitlog space exceeds the **commitlog_total_space_in_mb**, the 
memtable is put **in a queue** that is flushed to disk. The queue can be configured with the **memtable_heap_space_in_mb or memtable_offheap_space_in_mb**
setting in the cassandra.yaml file. If the data to be flushed exceeds the **memtable_cleanup_threshold**, Cassandra blocks writes until the next flush 
succeeds. You can manually flush a table using nodetool flushor nodetool drain (flushes memtables without listening for connections to other nodes). 
To reduce the commit log replay time, the recommended best practice is to flush the memtable before you restart the nodes. If a node stops working, r
eplaying the commit log restores to the memtable the writes that were there before it stopped.

Data in the commit log is purged after its corresponding data in the memtable is flushed to an SSTable on disk.

2. Storing data on disk in SSTables 

Memtables and SSTables are maintained **per table**. The commit log is **shared** among tables. SSTables are immutable, not written to again after the 
memtable is flushed. Consequently, a partition is typically stored across multiple SSTable files. A number of other SSTable structures exist to assist 
read operations:

For each SSTable, Cassandra creates these structures:

1. Data (Data.db) 
The SSTable data
2. Primary Index (Index.db) 
Index of the row keys with pointers to their positions in the data file
3. Bloom filter (Filter.db) 
A structure stored in memory that checks if row data exists in the memtable before accessing SSTables on disk
3. Compression Information (CompressionInfo.db) 
A file holding information about uncompressed data length, chunk offsets and other compression information
4. Statistics (Statistics.db) 
Statistical metadata about the content of the SSTable
5. Digest (Digest.crc32, Digest.adler32, Digest.sha1) 
A file holding adler32 checksum of the data file
6. CRC (CRC.db) 
A file holding the CRC32 for chunks in an a uncompressed file.
7. SSTable Index Summary (SUMMARY.db) 
A sample of the partition index stored in memory
8. SSTable Table of Contents (TOC.txt) 
A file that stores the list of all components for the SSTable TOC
9. Secondary Index (SI_.*.db) 
Built-in secondary index. Multiple SIs may exist per SSTable

The SSTables are files stored on disk. The naming convention for SSTable files has changed with Cassandra 2.2 and later to shorten the file path. 
The data files are stored in a data directory that varies with installation. For each keyspace, a directory within the data directory stores each table. 

For example, /data/data/ks1/cf1-5be396077b811e3a3ab9dc4b9ac088d/la-1-big-Data.db represents a data file. 

+ ks1 represents the keyspace name to distinguish the keyspace for streaming or bulk loading data. 
+ A hexadecimal string, 5be396077b811e3a3ab9dc4b9ac088d in this example, is appended to table names to represent unique table IDs.

Cassandra creates **a subdirectory for each table**, which allows you to symlink a table to a chosen physical drive or data volume. 
This provides the capability to move very active tables to faster media, such as SSDs for better performance, and also divides tables across all attached 
storage devices for better I/O balance at the storage layer.


## How is data maintained?

The Cassandra write process stores data in files called SSTables. **SSTables are immutable**. Instead of overwriting existing rows with inserts or updates, 
Cassandra writes **new timestamped versions** of the inserted or updated data in new SSTables. Cassandra does not perform deletes by removing the deleted data: 
instead, Cassandra **marks it with tombstones(墓碑)**.（在compact阶段）

Over time, Cassandra may write many versions of a row in different SSTables. Each version may have a unique set of columns stored with a different timestamp(
一般是Row的部分列更新会引起最新row的各列时间戳不一致). 
As SSTables accumulate, the distribution of data can require accessing more and more SSTables to retrieve a complete row.

1. cassandra 可能在多个SSTables写入同一个row的多个版本；
2. 同一个版本的row可能包含多个不同时间戳的columns；
以上两个因素会导致，cassandra在获取一行时需要读取多个SSTables，从而导致效率下降；

To keep the database healthy, Cassandra **periodically merges SSTables and discards old data**. This process is called compaction.

### Compaction 

Compaction将多个SSTables中同一个Row的所有版本的数据精简为一个新版本，删除旧版本的过程；

Compaction works on a collection of SSTables. From these SSTables, compaction collects all versions of each unique row and assembles one complete row, 
using the most up-to-date version (by timestamp) of each of the row's columns. The merge process is performant, because rows are sorted by partition key 
within each SSTable, and the merge process does not use random I/O. The new versions of each row is written to a new SSTable. The old versions, along with 
any rows that are ready for deletion, are left in the old SSTables, and are deleted as soon as pending reads are completed.

Compaction causes a temporary spike in disk space usage and disk I/O while old and new SSTables co-exist. As it completes, compaction frees up disk space 
occupied by old SSTables. It improves read performance by incrementally replacing old SSTables with compacted SSTables. Cassandra can read data directly 
from the new SSTable even before it finishes writing, instead of waiting for the entire compaction process to finish.

As Cassandra processes writes and reads, it replaces the old SSTables with new SSTables in the page cache. The process of caching the new SSTable, while 
directing reads away from the old one, is incremental — it does not cause a the dramatic cache miss. Cassandra provides **predictable high performance** even 
under heavy load.

### Compaction strategies 

Cassandra supports different compaction strategies, which control how which SSTables are chosen for compaction, and how the compacted rows are sorted into 
new SSTables. Each strategy has its own strengths. The sections that follow explain each of Cassandra's compaction strategies.

Although each of the following sections starts with a generalized recommendation, many factors complicate the choice of a compaction strategy. 
See Which compaction strategy is best?.

1. SizeTieredCompactionStrategy (STCS)

Recommended for write-intensive workloads.

The SizeTieredCompactionStrategy (STCS) initiates compaction when Cassandra has accumulated a set number (default: 4) of similar-sized SSTables. STCS merges 
these SSTables into one larger SSTable. As these larger SSTables accumulate, STCS merges these into even larger SSTables. At any given time, several SSTables
of varying sizes are present.

While STCS works well to compact **a write-intensive workload**, it makes reads slower because the merge-by-size process does not group data by rows. This 
makes it more likely that versions of a particular row may be spread over many SSTables. Also, STCS does not evict deleted data predictably because its 
trigger for compaction is SSTable size, and SSTables might not grow quickly enough to merge and evict old data. As the largest SSTables grow in size, 
the amount of disk space needed for both the new and old SSTables simultaneously during STCS compaction can outstrip a typical amount of disk space on a node.

Pros: Compacts write-intensive workload very well.
Cons: Can hold onto stale data too long. Amount of memory needed increases over time.

2. LeveledCompactionStrategy (LCS)

Recommended for read-intensive workloads.

The LeveledCompactionStrategy (LCS) alleviates some of the read operation issues with STCS. This strategy works with a series of levels. 

First, data in memtables is flushed to SSTables in the first level (L0). LCS compaction merges these first SSTables with larger SSTables in level L1.

The SSTables in levels greater than L1 are merged into SSTables with a size greater than or equal to **sstable_size_in_mb** (default: 160 MB). 
If a L1 SSTable stores data of a partition that is larger than L2, LCS moves the SSTable past L2 to the next level up.

In each of the levels above L0, LCS creates SSTables that are about the same size. Each level is 10X the size of the last level, so level L1 has 10X as 
many SSTables as L0, and level L2 has 100X as many. If the result of the compaction is more than 10 SSTables in level L1, the excess SSTables are moved to 
level L2.

The LCS compaction process guarantees that the SSTables within each level starting with L1 have non-overlapping data. For many reads, this process enables 
Cassandra to retrieve all the required data from only one or two SSTables. In fact, 90% of all reads can be satisfied from one SSTable. 
Since LCS does not compact L0 tables, however, resource-intensive reads involving many L0 SSTables may still occur.

At levels beyond L0, LCS requires less disk space for compacting — generally, 10X the fixed size of the SSTable. Obsolete data is evicted more often, 
so deleted data uses smaller portions of the SSTables on disk. However, LCS compaction operations take place more often and place more I/O burden on 
the node. For write-intensive workloads, the payoff of using this strategy is generally not worth the performance loss to I/O operations. 
In many cases, tests of LCS-configured tables reveal I/O saturation on writes and compactions.

Pros: Disk requirements are easier to predict. Read operation latency is more predictable. Stale data is evicted more frequently.
Cons: Much higher I/O utilization impacting operation latency

3. TimeWindowCompactionStrategy (TWCS)

Recommended for time series and expiring TTL workloads.

The TimeWindowCompactionStrategy (TWCS) is similar to DTCS with simpler settings. TWCS groups SSTables using a series of time windows. 
During compaction, TWCS applies STCS to uncompacted SSTables in the most recent time window. At the end of a time window, TWCS compacts all SSTables that 
fall into that time window into a single SSTable based on the SSTable maximum timestamp. Once the major compaction for a time window is completed, 
no further compaction of the data will ever occur. The process starts over with the SSTables written in the next time window.

As the figure shows, from 10 AM to 11 AM, the memtables are flushed from memory into 100MB SSTables. These SSTables are compacted into larger SSTables using
STCS. At 11 AM, all these SSTables are compacted into a single SSTable, and never compacted again by TWCS. At 12 NOON, the new SSTables created between 11 AM
and 12 NOON are compacted using STCS, and at the end of the time window the TWCS compaction repeats. Notice that each TWCS time window contains varying amounts
of data.

Note: For an animated explanation, see the Datastax Academy Time Window Compaction Strategy video.
The TWCS configuration has two main property settings:

compaction_window_unit: time unit used to define the window size (milliseconds, seconds, hours, and so on)
compaction_window_size: how many units per window (1,2,3, and so on)

The configuration for the above example: compaction_window_unit = ‘minutes’,compaction_window_size = 60

Pros: Used for time series data, stored in tables that use the default TTL for all data. Simpler configuration than that of DTCS.
Cons: Not appropriate if out-of-sequence time data is required, since SSTables will not compact as well. Also, not appropriate for data without a TTL,
as storage will grow without bound. Less fine-tuned configuration is possible than with DTCS.

# DateTieredCompactionStrategy (DTCS) 
Deprecated in Cassandra 3.0.8/3.8.

The DateTieredCompactionStrategy (DTCS) is similar to STCS. But instead of compacting based on SSTable size, DTCS compacts based on SSTable age. (Each column in an SSTable is marked with the timestamp at write time. As the age of an SSTable, DTCS uses the oldest (minimum) timestamp of any column the SSTable contains.)

Configuring the DTCS time window ensures that new and old data are not mixed in merged SSTables. In fact, using Time-To-Live (TTL) timestamps, DTCS can eject whole SSTables containing expired data. This strategy often generates similar-sized SSTables if time series data is ingested at a steady rate.

DTCS compacts SSTables into larger tables, as with STCS, when the system accumulates a configurable number of SSTables within a configurable time interval. However, DTCS skips compacting SSTables that reach a configurable age. This logic reduces the number of times data is rewritten. Queries that ask for data in a particular last time interval, such as an hour, can be executed very efficiently on DTCS-compacted SSTables (particularly if the requested time interval is coordinated with the configured interval for compaction).

One use case that can cause difficulty with this strategy is out-of-sequence writing. For example, an operation that writes a timestamped record with a timestamp outside the current time window. Read repairs can inject out-of-sequence timestamps, so be sure to turn off read repairs when using DTCS. For more information, see DateTieredCompactionStrategy: Notes from the Field.

Pros: Specifically designed for time series data, stored in tables that use the default TTL. DTCS is a better choice when fine-tuning is required to meet space-related SLAs.

Cons: Insertion of records out of time sequence (by repairs or hint replaying) can increase latency or cause errors. In some cases, it may be necessary to turn off read repair and carefully test and control the use of TIMESTAMP options in BATCH, DELETE, INSERT and UPDATE CQL commands.

## Which compaction strategy is best? 

# How is data updated?

Cassandra treats **each new row as an upsert**: if the new row has the same primary key as that of an existing row, Cassandra processes it as an update to the 
existing row.

During a write, Cassandra adds each new row to the database without checking on whether a duplicate record exists. This policy makes it possible that many 
versions of the same row may exist in the database. For more details about writes, see How is data written?

Periodically, the rows stored in memory are streamed to disk into structures called SSTables. At certain intervals, Cassandra **compacts** smaller SSTables into 
larger SSTables. If Cassandra encounters two or more versions of the same row during this process, Cassandra only writes the most recent version to the new 
SSTable. After compaction, Cassandra drops the original SSTables, deleting the outdated rows.

Most Cassandra installations store replicas of each row on two or more nodes. **Each node performs compaction independently**. This means that even though 
out-of-date versions of a row have been dropped from one node, they may still exist on another node.

This is why Cassandra **performs another round of comparisons during a read process**. When a client requests data with a particular primary key, Cassandra 
retrieves many versions of the row from one or more replicas. The version with the most recent timestamp is the only one returned to the client 
("last-write-wins").

Note: Some database operations may only write partial updates of a row, so some versions of a row may include some columns, but not all. 
During a compaction or write, Cassandra **assembles a complete version of each row from the partial updates**, using the most recent version of each column.

## How is data deleted?

Cassandra's processes for deleting data are designed to improve performance, and to work with Cassandra's built-in properties for data distribution 
and fault-tolerance.

Cassandra treats a delete as an insert or upsert. The data being added to the partition in the DELETE command is a deletion marker called a tombstone. 
The tombstones go through Cassandra's write path, and are written to SSTables on one or more nodes. 
The key difference feature of a tombstone: it has a built-in expiration date/time. At the end of its expiration period (for details see below) 
the tombstone is deleted as part of Cassandra's normal compaction process.

You can also mark a Cassandra record (row or column) with a time-to-live value. After this amount of time has ended, Cassandra marks the record with a 
tombstone, and handles it like other tombstoned records.

### Deletion in a distributed system 

In a multi-node cluster, Cassandra can store replicas of the same data on two or more nodes. This helps prevent data loss, but it complicates the delete 
process. If a node receives a delete for data it stores locally, the node tombstones the specified record and tries to **pass the tombstone to other nodes** 
containing replicas of that record. But if one replica node is unresponsive at that time, it does not receive the tombstone immediately, so it still contains
the pre-delete version of the record. If the tombstoned record has already been deleted from the rest of the cluster befor that node recovers, Cassandra treats
the record on the recovered node as new data, and propagates it to the rest of the cluster. This kind of deleted but persistent record is called a **zombie**.

To prevent the reappearance of zombies, Cassandra gives each tombstone **a grace period**. The purpose of the grace period is to give unresponsive nodes time 
to recover and process tombstones normally. If a client writes a new update to the tombstoned record during the grace period, Cassandra overwrites the 
tombstone. If a client sends a read for that record during the grace period, Cassandra disregards the tombstone and retrieves the record from other replicas 
if possible.

为了防止出现zombies，cassandra给每个tombstone分配一个grace时间，在这个时间内，client可以更新tombstone的recode，cassandra将overwrite该tombstone，client读该tombstone记录的话，
cassandra将丢弃该tombstone并从其它replicas获取记录；

When an unresponsive node recovers, Cassandra uses **hinted handoff** to replay the database mutations the node missed while it was down. 
Cassandra **does not replay a mutation for a tombstoned record during its grace period**. 

But if the node does not recover until after the grace period ends, Cassandra may miss the deletion.

**After the tombstone's grace period ends, Cassandra deletes the tombstone during compaction.**

The grace period for a tombstone is set by the property **gc_grace_seconds**. Its default value is 864000 seconds (ten days). Each table can have its own value 
for this property.

### More about Cassandra deletes 

Details:

1. The expiration date/time for a tombstone is the date/time of its creation **plus** the value of the table property gc_grace_seconds.
2. Cassandra also supports Batch data insertion and updates. This procedure also introduces the danger of replaying a record insertion after that record has 
been removed from the rest of the cluster. Cassandra does not replay a batched mutation for a tombstoned record that is still within its grace period.
3. On a single-node cluster, you can set gc_grace_seconds to 0 (zero).
4. To completely prevent the reappearance of zombie records, run nodetool repair on a node after it recovers, and on each table every gc_grace_seconds.
5. If all records in a table are given a TTL at creation, and all are allowed to expire and not deleted manually, it is not necessary to run nodetool repair for that table on a regular basis.
6. If you use the SizeTieredCompactionStrategy or DateTieredCompactionStrategy, you can delete tombstones immediately by manually starting the compaction process.
CAUTION:
If you force compaction, Cassandra may create one very large SSTable from all the data. Cassandra will not trigger another compaction for a long time. The data in the SSTable created during the forced compaction can grow very stale during this long period of non-compaction.
7. Cassandra allows you to set a default_time_to_live property for an entire table. Columns and rows marked with regular TTLs are processed as described above; but when a record exceeds the table-level TTL, Cassandra deletes it immediately, without tombstoning or compaction.
8. Cassandra supports immediate deletion through the DROP KEYSPACE and DROP TABLE statements.


## How are indexes stored and updated?

**Secondary indexes** are used to filter a table for data stored in non-primary key columns. For example, a table storing cyclist names and ages using the 
last name of the cyclist as the primary key might have a secondary index on the age to allow queries by age. Querying to match a non-primary key column is an 
anti-pattern, as querying should always result in a continuous slice of data retrieved from the table.

If the table rows are stored based on last names, the table may be spread across several partitions stored on different nodes. Queries based on a particular 
range of last names, such as all cyclists with the last name Matthews will retrieve sequential rows from the table, but a query based on the age, such as all 
cyclists who are 28, will require all nodes to be queried for a value. Non-primary keys play no role in ordering the data in storage, thus querying for a 
particular value of a non-primary key column results in scanning all partitions. 

Scanning all partitions generally results in a prohibitive read latency, and is not allowed.

Secondary indexes can be built for a column in a table. These indexes are **stored locally** on each node in a hidden table and built in a background process. 
If a secondary index is used in a query that is not restricted to a particular partition key, the query will have prohibitive read latency because all nodes 
will be queried. A query with these parameters is only allowed if the query option ALLOW FILTERING is used. This option is not appropriate for production 
environments. 

If a query includes **both a partition key condition and a secondary index column** condition, the query will be successful because the query can be directed to 
a single node partition.

This technique, however, does not guarantee trouble-free indexing, so know when and when not to use an index. In the example shown above, an index on the age 
could be used, but a better solution is to create a materialized view or additional table that is ordered by age.

As with relational databases, keeping indexes up to date uses processing time and resources, so unnecessary indexes should be avoided. 
When a column is updated, the index is updated as well. If the old column value still exists in the memtable, which typically occurs when updating a small 
set of rows repeatedly, Cassandra removes the corresponding obsolete index entry; otherwise, the old entry remains to be purged by compaction. If a read 
sees a stale index entry before compaction purges it, the reader thread invalidates it.

## How is data read?

To satisfy a read, Cassandra must **combine results from the active memtable and potentially multiple SSTables**.

Cassandra processes data at several stages on the read path to discover where the data is stored, starting with the data in the memtable and finishing with SSTables:
1. Check the memtable
2. Check row cache, if enabled
3. Checks Bloom filter
4. Checks partition key cache, if enabled
5. Goes directly to the compression offset map if a partition key is found in the partition key cache, or checks the partition summary if not
    If the partition summary is checked, then the partition index is accessed
6. Locates the data on disk using the compression offset map
7. Fetches the data from the SSTable on disk


1. Memtable

If the memtable has the desired partition data, then the data is read and then merged with the data from the SSTables. The SSTable data is accessed as 
shown in the following steps.

2. Row Cache

Typical of any database, reads are fastest when the most in-demand data fits into memory. The operating system page cache is best at improving performance, 
although the row cache can provide some improvement for very read-intensive operations, where read operations are 95% of the load. 

Row cache is contra-indicated for write-intensive operations. The row cache, if enabled, stores a subset of the partition data stored on disk in the SSTables
 in memory. In Cassandra 2.2 and later, it is stored in fully **off-heap memory** using a new implementation that relieves garbage collection pressure in the 
 JVM. The subset stored in the row cache use a configurable amount of memory for a specified period of time. The row cache uses LRU (least-recently-used) 
 eviction to reclaim memory when the cache has filled up.

The row cache size is configurable, as is the number of rows to store. Configuring the number of rows to be stored is a useful feature, making a 
"Last 10 Items" query very fast to read. If row cache is enabled, desired partition data is read from the row cache, potentially saving two seeks to disk 
for the data. The rows stored in row cache are frequently accessed rows that are merged and saved to the row cache from the SSTables as they are accessed.
After storage, the data is available to later queries. **The row cache is not write-through**. If a write comes in for the row, the cache for that row is 
invalidated and is not cached again until the row is read. Similarly, if a partition is updated, the entire partition is evicted from the cache. 

When the desired partition data is **not found** in the row cache, then the Bloom filter is checked.

3. Bloom Filter

First, Cassandra checks the Bloom filter to discover **which SSTables are likely to have the request partition data**. The Bloom filter is stored in off-heap
memory. **Each SSTable has a Bloom filter associated with it**. A Bloom filter can establish that a SSTable does not contain certain partition data.
A Bloom filter can also find the likelihood that partition data is stored in a SSTable. It speeds up the process of partition key lookup by narrowing the pool 
of keys. However, because the Bloom filter is a probabilistic function, it can result in false positives. Not all SSTables identified by the Bloom filter will
have data. 

**If the Bloom filter does not rule out an SSTable, Cassandra checks the partition key cache**

The Bloom filter grows to approximately 1-2 GB per billion partitions. In the extreme case, you can have one partition per row, so you can easily have 
billions of these entries on a single machine. The Bloom filter is tunable if you want to trade memory for performance.

4. Partition Key Cache

The partition key cache, if enabled, stores a cache of the partition index in off-heap memory. The key cache uses a small, configurable amount of memory, and each "hit" saves one seek during the read operation. If a partition key is found in the key cache can go directly to the compression offset map to find the compressed block on disk that has the data. The partition key cache functions better once warmed, and can greatly improve over the performance of cold-start reads, where the key cache doesn't yet have or has purged the keys stored in the key cache. It is possible to limit the number of partition keys saved in the key cache, if memory is very limited on a node. If a partition key is not found in the key cache, then the partition summary is searched.

The partition key cache size is configurable, as are the number of partition keys to store in the key cache.

5. Partition Summary

The partition summary is an off-heap in-memory structure that stores **a sampling of the partition index**. A partition index contains all partition keys, whereas a partition summary samples every X keys, and maps the location of every Xth key's location in the index file. For example, if the partition summary is set to sample every 20 keys, it will store the location of the first key as the beginning of the SSTable file, the 20th key and its location in the file, and so on. While not as exact as knowing the location of the partition key, the partition summary can shorten the scan to find the partition data location. After finding the range of possible partition key values, the partition index is searched.

By configuring the sample frequency, you can trade memory for performance, as the more granularity the partition summary has, the more memory it will use. The sample frequency is changed using the index interval property in the table definition. A fixed amount of memory is configurable using the index_summary_capacity_in_mb property, and defaults to 5% of the heap size.

6. Partition Index

The partition index resides on disk and stores **an index of all partition keys mapped to their offset**. If the partition summary has been checked for a range of partition keys, now the search passes to the partition index to seek the location of the desired partition key. A single seek and sequential read of the columns over the passed-in range is performed. Using the information found, the partition index now goes to the compression offset map to find the compressed block on disk that has the data. If the partition index must be searched, two seeks to disk will be required to find the desired data.

7. Compression offset map

The compression offset map stores pointers to the exact location on disk that the desired partition data will be found. It is stored in off-heap memory and 
is accessed by either the partition key cache or the partition index. The desired compressed partition data is fetched from the correct SSTable(s) once the 
compression offset map identifies the disk location. The query receives the result set.

Note: Within a partition, all rows are not equally expensive to query. The very beginning of the partition (the first rows, clustered by your key definition) is slightly less expensive to query because there is no need to consult the partition-level index.
The compression offset map grows to 1-3 GB per terabyte compressed. The more you compress data, the greater number of compressed blocks you have and the larger the compression offset table. Compression is enabled by default even though going through the compression offset map consumes CPU resources. Having compression enabled makes the page cache more effective, and typically, almost always pays off.

## How do write patterns affect reads?

It is important to consider how the write operations will affect the read operations in the cluster. The type of **compaction strategy** Cassandra performs on 
your data is configurable and can significantly affect read performance. Using the SizeTieredCompactionStrategy or DateTieredCompactionStrategy tends to 
cause data fragmentation when rows are frequently updated. The LeveledCompactionStrategy (LCS) was designed to prevent fragmentation under this condition.

# Data consistency 

## How are consistent read and write operations handled?

**Consistency refers to how up-to-date and synchronized all replicas of a row of Cassandra data are at any given moment.**

**Ongoing repair operations** in Cassandra ensure that all replicas of a row will **eventually be consistent**. Repairs work to decrease the variability in 
replica data, but constant data traffic through a widely distributed system can lead to inconsistency (stale data) at any given time. Cassandra is a AP system
according to the CAP theorem, providing high availability and partition tolerance. Cassandra does have flexibility in its configuration, though, and can 
perform more like a CP (consistent and partition tolerant) system according to the CAP theorem, depending on the application requirements. 

Two consistency features are **tunable consistency** and **linearizable consistency**.

### Tunable consistency 

To ensure that Cassandra can provide the proper levels of consistency for its reads and writes, Cassandra extends the concept of eventual consistency 
by offering tunable consistency. You can tune Cassandra's consistency level **per-operation**, or set it globally for a cluster or datacenter. You can vary 
the consistency for individual read or write operations so that the data returned is more or less consistent, as required by the client application. 
This allows you to make Cassandra act more like a CP (consistent and partition tolerant) or AP (highly available and partition tolerant) system according 
to the CAP theorem, depending on the application requirements.

Note: It is not possible to "tune" Cassandra into a completely CA system. See You Can't Sacrifice Partition Tolerance for a more detailed discussion.
There is a tradeoff between operation latency and consistency: higher consistency incurs higher latency, lower consistency permits lower latency. 
You can control latency by tuning consistency.

The consistency level determines **the number of replicas that need to acknowledge the read or write operation success** to the client application. 
For read operations, the read consistency level specifies how many replicas must respond to a read request before returning data to the client application.

If a read operation reveals inconsistency among replicas, Cassandra **initiates a read repair** to update the inconsistent data.

For write operations, the write consistency level specified how many replicas must respond to a write request before the write is considered successful. 
Even at low consistency levels, Cassandra **writes to all replicas** of the partition key, including replicas in other datacenters. The write consistency 
level just specifies when the coordinator can report to the client application that the write operation is considered completed. Write operations will use 
**hinted handoffs** to ensure the writes are completed when replicas are down or otherwise not responsive to the write request.

Typically, a client specifies a consistency level that is less than the replication factor specified by the keyspace. Another common practice is to write at 
a consistency level of **QUORUM** and read at a consistency level of QUORUM. The choices made depend on the client application's needs, and Cassandra provides 
maximum flexibility for application design.

### Linearizable consistency 

In ACID terms, linearizable consistency (or serial consistency) is a serial (immediate) isolation level for lightweight transactions. Cassandra does not use 
employ traditional mechanisms like locking or transactional dependencies when concurrently updating multiple rows or tables.

However, some operations must be performed in sequence and not interrupted by other operations. For example, duplications or overwrites in user account 
creation can have serious consequences. Situations like race conditions (two clients updating the same record) can introduce inconsistency across replicas. 
Writing with **high consistency does nothing to reduce this**. You can apply linearizable consistency to a unique identifier, like the userID or email address, 
although is not required for all aspects of the user's account. Serial operations for these elements can be implemented in Cassandra with 
**the Paxos consensus protocol**, which uses a quorum-based algorithm. Lightweight transactions can be implemented without the need for a master database 
or two-phase commit process.

Lightweight transaction write operations use the serial consistency level for Paxos consensus and the regular consistency level for the write to the table. 
For more information, see Lightweight Transactions.

### Calculating consistency

Reliability of read and write operations depends on the consistency used to verify the operation. Strong consistency can be guaranteed when the following 
condition is true:
R + W > N
where
R is the consistency level of read operations
W is the consistency level of write operations
N is the number of replicas

If the replication factor is 3, then the consistency level of the reads and writes combined must be at least 4. For example, read operations using 2 out of 3 
replicas to verify the value, and write operations using 2 out of 3 replicas to verify the value will result in strong consistency. If fast write operations 
are required, but strong consistency is still desired, the write consistency level is lowered to 1, but now read operations have to verify a matched value on all 3 replicas. Writes will be fast, but reads will be slower.

Eventual consistency occurs if the following condition is true:
R + W =< N
where
R is the consistency level of read operations
W is the consistency level of write operations
N is the number of replicas

If the replication factor is 3, then the consistency level of the reads and writes combined are 3 or less. For example, read operations using QUORUM (2 out of 3 replicas) to verify the value, and write operations using ONE (1 out of 3 replicas) to do fast writes will result in eventual consistency. All replicas will receive the data, but read operations are more vulnerable to selecting data before all replicas write the data.

Additional consistency examples:

You do a write at ONE, the replica crashes one second later. The other messages are not delivered. The data is lost.
You do a write at ONE, and the operation times out. Future reads can return the old or the new value. You will not know the data is incorrect.
You do a write at ONE, and one of the other replicas is down. The node comes back online. The application will get old data from that node until the node gets the correct data or a read repair occurs.
You do a write at QUORUM, and then a read at QUORUM. One of the replicas dies. You will always get the correct data.

## How are Cassandra transactions different from RDBMS transactions? 

Cassandra does not use RDBMS ACID transactions with rollback or locking mechanisms, but instead offers atomic, isolated, and durable transactions with 
eventual/tunable consistency that lets the user decide how strong or eventual they want each transaction’s consistency to be.

As a non-relational database, Cassandra does not support joins or foreign keys, and consequently does not offer consistency in the ACID sense. For example, when moving money from account A to B the total in the accounts does not change. Cassandra supports atomicity and isolation at the row-level, but trades transactional isolation and atomicity for high availability and fast write performance. Cassandra writes are durable.

1.  Atomicity 

In Cassandra, a write operation is atomic at the partition level, meaning the insertions or updates of two or more rows in the same partition are treated as 
one write operation. A delete operation is also atomic at the partition level.

For example, if using a write consistency level of QUORUM with a replication factor of 3, Cassandra will replicate the write to all nodes in the cluster and 
wait for acknowledgement from two nodes. If the write fails on one of the nodes but succeeds on the other, Cassandra reports a failure to replicate the write 
on that node. However, the replicated write that succeeds on the other node is not automatically rolled back.

Cassandra uses **client-side timestamps** to determine the most recent update to a column. The latest timestamp always wins when requesting data, so if
multiple client sessions update the same columns in a row concurrently, the most recent update is the one seen by readers.

2. Isolation 

Cassandra write and delete operations are performed with **full row-level isolation**. This means that a write to a row within a single partition on a single 
node is only visible to the client performing the operation – the operation is restricted to this scope until it is complete. All updates in a batch operation
belonging to a given partition key have the same restriction. However, a Batch operation is not isolated if it includes changes to more than one partition.

3. Durability 

Writes in Cassandra are durable. All writes to a replica node are recorded both in memory and in a commit log on disk before they are acknowledged as a 
success. If a crash or server failure occurs before the memtables are flushed to disk, the commit log is replayed on restart to recover any lost writes. 
In addition to the local durability (data immediately written to disk), the replication of data on other nodes strengthens durability.

You can manage the local durability to suit your needs for consistency using the **commitlog_sync option** in the cassandra.yaml file. 
Set the option to either periodic or batch.

### How do I accomplish lightweight transactions with linearizable consistency?

Distributed databases present a unique challenge when data must be strictly read and written in sequential order. In transactions for creating user accounts 
or transferring money, race conditions between two potential writes must be regulated to ensure that one write precedes the other. 

**In Cassandra, the Paxos consensus protocol is used to implement lightweight transactions that can handle concurrent operations.**

The Paxos protocol is implemented in Cassandra with **linearizable consistency, that is sequential consistency with real-time constraints.** 

Linearizable consistency ensures transaction isolation at a level similar to the serializable level offered by RDBMSs. This type of transaction is known as 
compare and set (CAS); replica data is compared and any data found to be out of date is set to the most consistent value. In Cassandra, the process combines 
the Paxos protocol with normal read and write operations to accomplish the compare and set operation.

The Paxos protocol is implemented as a series of phases:
1. Prepare/Promise
2. Read/Results
3. Propose/Accept
4. Commit/Acknowledge

These phases are actions that take place between a proposer and acceptors. Any node can be a proposer, and multiple proposers can be operating at the same time.
For simplicity, this description will use only one proposer. A proposer prepares by sending a message to a quorum of acceptors that includes a proposal number. 
Each acceptor promises to accept the proposal if the proposal number is the highest they have received. Once the proposer receives a quorum of acceptors who 
promise, the value for the proposal is read from each acceptor and sent back to the proposer. The proposer figures out which value to use and proposes the value
to a quorum of the acceptors along with the proposal number. Each acceptor accepts the proposal with a certain number if and only if the acceptor is not already
promised to a proposal with a high number. The value is committed and acknowledged as a Cassandra write operation if all the conditions are met.

These four phases require four round trips between a node proposing a lightweight transaction and any cluster replicas involved in the transaction. 
Performance will be affected. Consequently, reserve lightweight transactions for situations where concurrency must be considered.

Lightweight transactions will block other lightweight transactions from occurring, but will not stop normal read and write operations from occurring. 
Lightweight transactions use a timestamping mechanism different than for normal operations and mixing LWTs and normal operations can result in errors. 
If lightweight transactions are used to write to a row within a partition, only lightweight transactions for both read and write operations should be used. 
This caution applies to all operations, whether individual or batched. For example, the following series of operations can fail:

DELETE ...
INSERT .... IF NOT EXISTS
SELECT ....

The following series of operations will work:
DELETE ... IF EXISTS
INSERT .... IF NOT EXISTS
SELECT .....

### Reads with linearizable consistency

A SERIAL consistency level allows reading the current (and possibly uncommitted) state of data without proposing a new addition or update. 
If a SERIAL read finds an uncommitted transaction in progress, Cassandra performs a read repair as part of the commit.

## How do I discover consistency level performance? 

Before changing the consistency level on read and write operations, discover how your CQL commands are performing using **the TRACING command** in CQL. 
Using cqlsh, you can vary the consistency level and trace read and write operations. The tracing output includes latency times for the operations.

The CQL documentation includes a tutorial comparing consistency levels.

## How is the consistency level configured?

Consistency levels in Cassandra can be configured to manage availability versus data accuracy. You can configure consistency on a cluster, datacenter, or per
individual read or write operation. Consistency among participating nodes can be set globally and also controlled on a per-operation basis. Within cqlsh, 
use **CONSISTENCY**, to set the consistency level for all queries in the current cqlsh session. For programming client applications, set the consistency level 
using an appropriate driver. For example, using the Java driver, call QueryBuilder.insertInto with setConsistencyLevel to set a per-insert consistency level.

The consistency level **defaults to ONE** for all write and read operations.

### Write consistency levels 

This table describes the write consistency levels in strongest-to-weakest order.

Write Consistency Levels
Level	Description	Usage
1. ALL	A write must be written to the commit log and memtable on all replica nodes in the cluster for that partition.	Provides the highest consistency and the lowest availability of any other level.
EACH_QUORUM	Strong consistency. A write must be written to the commit log and memtable on a quorum of replica nodes in each datacenter.	Used in multiple datacenter clusters to strictly maintain consistency at the same level in each datacenter. For example, choose this level if you want a read to fail when a datacenter is down and the QUORUM cannot be reached on that datacenter.
2. QUORUM	A write must be written to the commit log and memtable on a quorum of replica nodes across all datacenters.	Used in either single or multiple datacenter clusters to maintain strong consistency across the cluster. Use if you can tolerate some level of failure.
3. LOCAL_QUORUM	Strong consistency. A write must be written to the commit log and memtable on a quorum of replica nodes in the same datacenter as the coordinator. Avoids latency of inter-datacenter communication.	Used in multiple datacenter clusters with a rack-aware replica placement strategy, such as NetworkTopologyStrategy, and a properly configured snitch. Use to maintain consistency locally (within the single datacenter). Can be used with SimpleStrategy.
4. ONE	A write must be written to the commit log and memtable of at least one replica node.	Satisfies the needs of most users because consistency requirements are not stringent.
5. TWO	A write must be written to the commit log and memtable of at least two replica nodes.	Similar to ONE.
6. THREE	A write must be written to the commit log and memtable of at least three replica nodes.	Similar to TWO.
7. LOCAL_ONE	A write must be sent to, and successfully acknowledged by, at least one replica node in the local datacenter.	In a multiple datacenter clusters, a consistency level of ONE is often desirable, but cross-DC traffic is not. LOCAL_ONE accomplishes this. For security and quality reasons, you can use this consistency level in an offline datacenter to prevent automatic connection to online nodes in other datacenters if an offline node goes down.
8. ANY	A write must be written to at least one node. If all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff has been written. If all replica nodes are down at write time, an ANY write is not readable until the replica nodes for that partition have recovered.	Provides low latency and a guarantee that a write never fails. Delivers the lowest consistency and highest availability.
Read consistency levels 

This table describes read consistency levels in strongest-to-weakest order.

### Read Consistency Levels
Level	Description	Usage
1. ALL	Returns the record after all replicas have responded. The read operation will fail if a replica does not respond.	Provides the highest consistency of all levels and the lowest availability of all levels.
2. EACH_QUORUM	Not supported for reads.
3. QUORUM	Returns the record after a quorum of replicas from all datacenters has responded.	Used in either single or multiple datacenter clusters to maintain strong consistency across the cluster. Ensures strong consistency if you can tolerate some level of failure.
4. LOCAL_QUORUM	Returns the record after a quorum of replicas in the current datacenter as the coordinator has reported. Avoids latency of inter-datacenter communication.	Used in multiple datacenter clusters with a rack-aware replica placement strategy ( NetworkTopologyStrategy) and a properly configured snitch. Fails when using SimpleStrategy.
5. ONE	Returns a response from the closest replica, as determined by the snitch. By default, a read repair runs in the background to make the other replicas consistent.	Provides the highest availability of all the levels if you can tolerate a comparatively high probability of stale data being read. The replicas contacted for reads may not always have the most recent write.
6. TWO	Returns the most recent data from two of the closest replicas.	Similar to ONE.
7. THREE	Returns the most recent data from three of the closest replicas.	Similar to TWO.
8. LOCAL_ONE	Returns a response from the closest replica in the local datacenter.	Same usage as described in the table about write consistency levels.
9. SERIAL	Allows reading the current (and possibly uncommitted) state of data without proposing a new addition or update. If a SERIAL read finds an uncommitted transaction in progress, it will commit the transaction as part of the read. Similar to QUORUM.	To read the latest value of a column after a user has invoked a lightweight transaction to write to the column, use SERIAL. Cassandra then checks the inflight lightweight transaction for updates and, if found, returns the latest data.
10. LOCAL_SERIAL	Same as SERIAL, but confined to the datacenter. Similar to LOCAL_QUORUM.	Used to achieve linearizable consistency for lightweight transactions.

### How QUORUM is calculated 

The QUORUM level writes to the number of nodes that make up a quorum. A quorum is calculated, and then rounded down to a whole number, as follows:

quorum = (sum_of_replication_factors / 2) + 1
The sum of all the replication_factor settings for each datacenter is the sum_of_replication_factors.

sum_of_replication_factors = datacenter1_RF + datacenter2_RF + . . . + datacentern_RF

## How is the serial consistency level configured?

Serial consistency levels in Cassandra can be configured to manage lightweight transaction isolation. Lightweight transactions have two consistency levels 
defined. 

The serial consistency level defines the consistency level of **the serial phase, or Paxos phase**, of lightweight transactions. 
The learn phase, which defines what read operations will be guaranteed to complete immediately if lightweight writes are occurring uses a normal consistency 
level. The serial consistency level is ignored for any query that is not a conditional update.

Serial consistency levels 

Serial Consistency Levels
Level	Description	Usage
1. SERIAL	Achieves linearizable consistency for lightweight transactions by preventing unconditional updates.	This consistency level is only for use with lightweight transaction. Equivalent to QUORUM.
2. LOCAL_SERIAL	Same as SERIAL but confined to the datacenter. A conditional write must be written to the commit log and memtable on a quorum of replica nodes in the same datacenter.	Same as SERIAL but used to maintain consistency locally (within the single datacenter). Equivalent to LOCAL_QUORUM.

## How are read requests accomplished?

There are three types of read requests that a **coordinator can send to a replica**:

1. A direct read request
2. A digest request
3. A background read repair request

In a direct read request, the coordinator node contacts one replica node. Then the coordinator sends **a digest request** to a number of replicas determined by 
the consistency level specified by the client. The digest request checks the data in the replica node to make sure it is up to date. Then the coordinator 
sends a digest request to **all remaining** replicas. If any replica nodes have out of date data, **a background read repair** request is sent. Read repair requests 
ensure that the requested row is made consistent on all replicas involved in a read query.

For a digest request the coordinator first contacts the replicas specified by the consistency level. The coordinator sends these requests to the replicas that
currently respond the fastest. The contacted nodes respond with a digest of the requested data; if multiple nodes are contacted, the rows from each replica 
are compared in memory for consistency. If they are not consistent, the replica having the most recent data (based on the timestamp) is used by the 
coordinator to forward the result back to the client. To ensure that all replicas have the most recent version of the data, read repair is carried out to 
update out-of-date replicas.

For illustrated examples of read requests, see Examples of read consistency levels.

### Rapid read protection using speculative_retry 

Rapid read protection allows Cassandra to still deliver read requests when the originally selected replica nodes are either down or taking too long to respond.
If the table has been configured with the **speculative_retry** property, the coordinator node for the read request will retry the request with **another replica** 
node if the original replica node exceeds a configurable timeout value to complete the read request.

Recovering from replica node failure with rapid read protection

Examples of read consistency levels
Read request examples with different consistency levels.


## How are write requests accomplished?

The coordinator sends a write request to **all replicas** that own the row being written. As long as all replica nodes are up and available, they will get the 
write regardless of the consistency level specified by the client. The write consistency level determines how many replica nodes must respond with a success 
acknowledgment in order for the write to be considered successful. Success means that the data was written to the commit log and the memtable as described in 
how data is written.

The coordinator node forwards the write to replicas of that row, and responds to the client once it receives write acknowledgments from the number of nodes 
specified by the consistency level. Exceptions:
+ If the coordinator cannot write to enough replicas to meet the requested consistency level, it throws an Unavailable Exception and does not perform any 
writes.
+ If there are enough replicas available but the required writes don't finish within the timeout window, the coordinator throws a Timeout Exception.

For example, in a single datacenter 10-node cluster with a replication factor of 3, an incoming write will go to all 3 nodes that own the requested row. 
If the write consistency level specified by the client is ONE, the first node to complete the write responds back to the coordinator, which then proxies the 
success message back to the client. A consistency level of ONE means that it is possible that 2 of the 3 replicas can miss the write if they happen to be down
at the time the request is made. If a replica misses a write, the row is made consistent later using one of **the built-in repair mechanisms: hinted handoff, read repair, or anti-entropy node repair.**


## Multiple datacenter write requests

In multiple datacenter deployments, Cassandra optimizes write performance by choosing one coordinator node. The coordinator node contacted by the client 
application forwards the write request **to one replica in each of the other datacenters**, with **a special tag** to forward the write to the other local replicas.

If the write consistency level is LOCAL_ONE or LOCAL_QUORUM, only the nodes in the same datacenter as the coordinator node must respond to the client request 
in order for the request to succeed. Use either LOCAL_ONE or LOCAL_QUORUM to reduce geographical latency lessen the impact on client write request response 
times.